#!/usr/bin/env python
# coding=utf-8
# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Fine-tuning a ðŸ¤— Transformers model on summarization and text-to-text classification
"""
# You can also adapt this script on your own summarization task. Pointers for this are left as comments.

import os
import argparse
import json
import math
import psutil
import os
import random
from pprint import pformat
import time

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

import nltk
import datasets
import evaluate

import transformers
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoModelForCausalLM,
    AutoTokenizer,
    SchedulerType,
    get_scheduler,
    set_seed,
)

from accelerate import Accelerator
from accelerate.utils import set_seed
from datasets import load_dataset

import wandb
from tqdm.auto import tqdm, trange
from loguru import logger

import peft
import adapters


import peft_comparison
import peft_comparison.utils
import peft_comparison.text2text_utils
import peft_comparison.mappings
from peft_comparison.collation import DataCollatorForSeq2SeqWithMetadata, DataCollatorForCausalLMWithMetadata

# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
datasets.utils.logging.set_verbosity_error()
transformers.utils.logging.set_verbosity_error()

nltk.data.find("tokenizers/punkt")

str2bool = lambda x: x.lower() in ["true", "1"]


def parse_args():
    parser = argparse.ArgumentParser(description="Finetune a transformers model on a summarization task")

    # Dataset Configuration
    parser.add_argument("--dataset_name", type=str, required=True, help="The name of the dataset to use via the datasets library.")
    parser.add_argument("--task_type", default=None, choices=["summarization", "classification"])
    parser.add_argument("--dataset_config_name", default=None, help="""
                        The configuration name of the dataset to use via the datasets library
                        E.g., for superglue/glue it would be one of:
                        "cola", "mnli", "mrpc", "qnli", "qqp", "rte", "sst2", "stsb", "wnli", "boolq", "cb", "copa", "multirc"
                        And for cnn_dailymail it can be "3.0.0"
                        Lookup huggingface.co/datasets for more information.
                        """)
    parser.add_argument("--max_source_length", type=int, default=512, help="The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.")
    parser.add_argument("--source_prefix", type=str, default="", help="A prefix to add before every source text, useful for T5 models.")
    parser.add_argument("--preprocessing_num_workers", type=int, default=8, help="The number of processes to use for the preprocessing.")
    parser.add_argument("--subsample_data", type=int, default=None, help="If passed, will subsample the dataset to this many examples. (debug only)")
    parser.add_argument("--skip_test_eval", action="store_true", help="If passed, will skip the final test evaluation (still final eval on validation).")

    # Target Text Configuration
    parser.add_argument("--max_target_length", type=int, default=128, help="The maximum total sequence length for target text after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded during evaluate and predict.")
    parser.add_argument("--val_max_target_length", type=int, default=None, help="The maximum total sequence length for validation target text after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded. Will default to max_target_length. This argument is also used to override the max_length param of model.generate, which is used during evaluate and predict.")
    parser.add_argument("--num_beams", type=int, default=None, help="Number of beams to use for evaluation. This argument will be passed to model.generate, which is used during evaluate and predict.")
    parser.add_argument("--pad_to_max_length", action="store_true", help="If passed, pad all samples to max_length. Otherwise, dynamic padding is used.")
    parser.add_argument("--max_eval_steps_durig_validation", type=int, default=100, help="Maximum number of evaluation steps to perform during validation. Useful to save time when you don't need to run the full validation set.")

    # Model Configuration
    parser.add_argument("--model_name_or_path", type=str, default="t5-base", help="Path to pretrained model or model identifier from huggingface.co/models.")

    # Batch Configuration
    parser.add_argument("--per_device_train_batch_size", type=int, default=2, help="Batch size per device for the training dataloader.")
    parser.add_argument("--per_device_eval_batch_size", type=int, default=None, help="Batch size per device for the evaluation dataloader.")
    parser.add_argument("--total_batch_size", type=int, default=32, help="Total batch size per_device_batch_size * num_devices * gradient_accumulation")

    # Training Configuration
    parser.add_argument("--gradient_accumulation_steps", type=int, default=None, help="Number of updates steps to accumulate before performing a backward/update pass.")
    parser.add_argument("--learning_rate", type=float, default=2e-4, help="Initial learning rate after the potential warmup period to use.")
    parser.add_argument("--lr_scheduler_warmup_percent", type=float, default=0.06, help="Percentage of steps for the warmup in the lr scheduler.")
    parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
    parser.add_argument("--num_train_epochs", type=int, default=1, help="Total number of training epochs to perform.")
    parser.add_argument("--max_train_steps", type=int, default=None, help="Total number of training steps to perform. If provided, overrides num_train_epochs.")
    parser.add_argument("--min_train_steps", type=int, default=100)
    parser.add_argument("--eval_every_steps", type=int, default=None, help="Evaluate model after these many steps.")
    parser.add_argument("--lr_scheduler_type", type=SchedulerType, default="linear", help="The scheduler type to use, choices: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup")
    parser.add_argument("--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler.")
    parser.add_argument("--gradient_checkpointing", default=False, action="store_true", help="only used for full_tuning")

    # Output and Tracking
    parser.add_argument("--output_dir", type=str, default=None, help="Where to store the final model.")
    parser.add_argument("--seed", type=int, default=0, help="A seed for reproducible training.")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="If the training should continue from a checkpoint folder.")
    parser.add_argument("--skip_initial_eval", action="store_true", help="If passed, will skip the initial evaluation before training.")
    parser.add_argument("--eval_throughput_estimation", action="store_true", help="If passed, will only run the initial evaluation and no training (used to estimate test throughput cheaply)")
    parser.add_argument("--wandb_team", type=str, default="text_machine_lab_babylm", help="Name of the wandb team to use")

    # PEFT Configuration
    parser.add_argument("--adapter_config_string", default=None, type=str, help="The adapter config string to use for adapter-transformers")
    parser.add_argument("--num_layers", default=1, type=int, help="Number of layers to unfreeze")

    # Memory Management
    parser.add_argument("--load_in_8bit", action="store_true", help="Enable 8bit quantization.")
    parser.add_argument("--load_in_4bit", action="store_true", help="Enable 4bit quantization.")
    parser.add_argument("--torch_dtype", default=None, help="This sets the dtype of the remaining non quantized layers. 'bitsandbytes' library suggests to set the value to 'torch.float16' for 8 bit model and use the same dtype as the compute dtype for 4 bit model")

    # Weight and Biases Configuration
    parser.add_argument("--wandb_project", type=str, default="PEFT_Comparison", help="Name to be given to Weight and Biases logging repository")
    parser.add_argument("--tags", type=str, default=None, help="Tags to be given to individual runs in WandB repository, e.g. 'trial, t5-base, classification'")
    parser.add_argument("--wandb_name", type=str, default=None, help="Display name for the run")

    # Misc
    parser.add_argument("--verbosity", type=int, default=1, help="verbosity of the logger (1 or 2 for now)")
    parser.add_argument("--profile", action="store_true", help="Enable profiler and log profile traces to pytorch_tensorboard")
    args = parser.parse_args()

    # Sanity checks
    if args.dataset_name is None:
        raise ValueError("Need dataset name")

    if args.tags is not None:
        args.tags = args.tags.split(",")

    if args.val_max_target_length is None:
        args.val_max_target_length = args.max_target_length

    # this allows to effectively mitigate OOM issues with beam search
    if args.per_device_eval_batch_size is None and args.num_beams < 5:
        args.per_device_eval_batch_size = args.per_device_train_batch_size

    if args.per_device_eval_batch_size is None and args.num_beams >= 5:
        args.per_device_eval_batch_size = max(1, args.per_device_train_batch_size // 2)

    if args.dataset_name == "cnn_dailymail":
        args.dataset_config_name = "3.0.0"

    if args.task_type is None:
        if args.dataset_name in peft_comparison.mappings.summarization_name_mapping:
            args.task_type = "summarization"
        elif args.dataset_config_name in peft_comparison.mappings.task_to_keys:
            args.task_type = "classification"
        else:
            raise ValueError(f"--task_type must be specified for unknown dataset name {args.dataset_name}. "
                             "But honestly, probably this dataset is not supported anyway. "
                             "To support dataset you need to at least include it into "
                             "peft_comparison.mappings.summarization_name_mapping or peft_comparison.mappings.task_to_keys "
                             "and add a postprocessing function")

    args.decoder_only = False if "t5" in args.model_name_or_path else True

    if args.output_dir == "None":  # I am not sure why this happens
        args.output_dir = None

    if args.torch_dtype is None:
        args.torch_dtype = torch.bfloat16
    else:
        if args.torch_dtype == "float32":
            args.torch_dtype = torch.float32
        elif args.torch_dtype == "bfloat16":
            args.torch_dtype = torch.bfloat16
        else:
            raise ValueError(f"Unknown torch dtype {args.torch_dtype}")

    if args.adapter_config_string == "full_tuning":
        if args.load_in_8bit or args.load_in_4bit:
            raise ValueError(f"Full tuning can't be applied to quantized models")

    return args


def preprocess_adapter_config_string_for_t5(adapter_config_string, model_config):
    if "prefix_tuning" in adapter_config_string:
        if "[" in adapter_config_string:
            logger.info(f"Custom config string provided: {adapter_config_string}")
            logger.info(f"Using the custom config string as is, if you get an error related to kv_size, please check the config string")
            logger.info(f"It should contain kv_size for T5-3B and T5-11B")
            return adapter_config_string

        logger.info(f"Adding [kv_size={model_config.d_kv}] to the adapter config string")
        adapter_config_string += f"[kv_size={model_config.d_kv}]"
        logger.info(f"Using adapter config string: {adapter_config_string}")
        return adapter_config_string

    if adapter_config_string == "unipelt":
        logger.info("Building default unipelt config for T5. Non-default unipelt configuratoins need to be provided explicitly")
        _lora = "lora[r=8,use_gating=True]"
        _prefix = f"prefix_tuning[prefix_length=10,flat=True,use_gating=True,kv_size={model_config.d_kv}]"
        _adapter = "houlsby[use_gating=True]"
        adapter_config_string = f"{_lora}|{_prefix}|{_adapter}"
        logger.info(f"Using adapter config string: {adapter_config_string}")
        return adapter_config_string

    if adapter_config_string == "mam":
        logger.info("Building default MAM config for T5. Non-default mam configuratoins need to be provided explicitly")
        _prefix = f"prefix_tuning[bottleneck_size=800,kv_size={model_config.d_kv}]"
        _adapter = "scaled_parallel"
        adapter_config_string = f"{_prefix}|{_adapter}"
        logger.info(f"Using adapter config string: {adapter_config_string}")
        return adapter_config_string

    if adapter_config_string == "mam-flat":
        logger.info("Building default MAM config for T5. Non-default mam configuratoins need to be provided explicitly")
        _prefix = f"prefix_tuning[flat=True,kv_size={model_config.d_kv},flat=True]"
        _adapter = "scaled_parallel"
        adapter_config_string = f"{_prefix}|{_adapter}"
        logger.info(f"Using adapter config string: {adapter_config_string}")
        return adapter_config_string

    return adapter_config_string


def get_model(args, device):
    if "t5" in args.model_name_or_path and args.source_prefix is None and args.task_type == "summarization":
        logger.warning(
            "You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with "
            "`--source_prefix 'summarize: ' `"
        )

    # model
    model_class = AutoModelForSeq2SeqLM
    constructor_kwargs = {
        "torch_dtype": args.torch_dtype,
        "load_in_4bit": args.load_in_4bit,
        "load_in_8bit": args.load_in_8bit,
        "low_cpu_mem_usage": True,
    }
    is_llama = "llama" in args.model_name_or_path.lower()
    if is_llama:
        logger.info("Using LLaMA model")
        model_class = AutoModelForCausalLM
        constructor_kwargs["use_flash_attention_2"] = True

    logger.info(f"Loading model with kwargs {pformat(constructor_kwargs)}")

    model = model_class.from_pretrained(args.model_name_or_path, **constructor_kwargs)
    if args.gradient_checkpointing:
        logger.info("Using gradient checkpointing")
        model.gradient_checkpointing_enable()

    # add peft modules
    if not args.adapter_config_string in ["bitfit", "ln_tuning", "attn_tuning", "full_tuning", "layer_unfreeze"]:
        adapters.init(model)

        # freeze all parameters
        for param in model.parameters():
            param.requires_grad = False

        # add PEFT
        if not is_llama:
            # T5-3B and T5-11B have a weird setup where key_size != hidden_size // num_heads
            args.adapter_config_string = preprocess_adapter_config_string_for_t5(
                adapter_config_string=args.adapter_config_string,
                model_config=model.config,
            )

        model.add_adapter("adapter", config=args.adapter_config_string, set_active=True)
        model.train()
        model.train_adapter("adapter")
        for name, module in model.named_modules():
            if "adapter" in name:
                module.to(device)
                for param in module.parameters():
                    param.requires_grad = True

    elif args.adapter_config_string == "layer_unfreeze":
        logger.info(f"Unfreezing last {args.num_layers} layers")
        # customized to unfreeze last N layers
        for param in model.parameters():
            param.requires_grad = False

        for block in model.decoder.block[-args.num_layers:]:
            for param in block.parameters():
                param.requires_grad = True

    elif args.adapter_config_string == "bitfit":
        # freeze all but bias parameters
        for name, param in model.named_parameters():
            if not "bias" in name:
                param.requires_grad = False

    elif args.adapter_config_string == "ln_tuning":
        # freeze all but LN parameters
        for name, param in model.named_parameters():
            # LlaMa layer norm key: input_layernorm, post_attention_layernorm
            # T5 layer norm key:    layer_norm
            if not (("_layernorm" in name) or ("layer_norm" in name)):
                param.requires_grad = False

    elif args.adapter_config_string == "attn_tuning":
        NotImplementedError("Attention tuning is not implmented yet")

    elif args.adapter_config_string == "full_tuning":
        if args.load_in_8bit or args.load_in_4bit:
            logger.error(f"Full tuning can't be applied to quantized models")

    # send to device if not quantized
    if (not args.load_in_8bit) and (not args.load_in_4bit):
        model = model.to(dtype=args.torch_dtype)

    # adapter is not yet on the device
    if args.load_in_8bit or args.load_in_4bit:
        for name, module in model.named_modules():
            if "adapter" in name:
                module.to(device=device, dtype=args.torch_dtype)

    return model


def get_model_hf(args, device):
    if "t5" in args.model_name_or_path and args.source_prefix is None and args.task_type == "summarization":
        logger.warning(
            "You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with "
            "`--source_prefix 'summarize: ' `"
        )

    model_class = AutoModelForSeq2SeqLM
    task_type = peft.TaskType.SEQ_2_SEQ_LM
    is_llama = "llama" in args.model_name_or_path.lower()
    if is_llama:
        logger.info("Using LLaMA model")
        model_class = AutoModelForCausalLM
        task_type = peft.TaskType.CAUSAL_LM

    model = model_class.from_pretrained(
        args.model_name_or_path,
        torch_dtype=args.torch_dtype,
        load_in_4bit=args.load_in_4bit,
        load_in_8bit=args.load_in_8bit,
        **({"use_flash_attention_2": True} if is_llama else {}),
        low_cpu_mem_usage=True,
    )

    if args.load_in_8bit:
        model = peft.prepare_model_for_int8_training(model)

    method, method_kwargs = peft_comparison.utils.parse_config_string(args.adapter_config_string)
    if method not in peft_comparison.mappings.hf_adapter_config_string_to_peft_args:
        raise ValueError(f"Unknown method {method}, config string={args.adapter_config_string}")

    default_kwargs = peft_comparison.mappings.hf_adapter_config_string_to_peft_args[method]
    method_kwargs = default_kwargs | method_kwargs
    logger.info(f"Adapter mentod_kwargs: {method_kwargs}")

    if method in ["hf_lora", "hf_lora_all"]:
        peft_config = peft.LoraConfig(
            task_type=task_type,
            inference_mode=False,
            **method_kwargs,
        )
    elif method == "hf_krona":
        peft_config = peft.LoKrConfig(
            task_type=task_type,
            **method_kwargs,
        )
    elif method == "hf_loha":
        peft_config = peft.LoHaConfig(
            task_type=task_type,
            **method_kwargs,
        )
    elif method == "hf_ia3":
        peft_config = peft.IA3Config(
            task_type=task_type,
            **method_kwargs,
        )
    elif method == "hf_prompt_tuning_L":
        peft_config = peft.PromptTuningConfig(
            task_type=task_type,
            tokenizer_name_or_path=args.model_name_or_path,
            **method_kwargs,
        )
    elif method == "hf_prompt_tuning_3b":
        peft_config = peft.PromptTuningConfig(
            task_type=task_type,
            tokenizer_name_or_path=args.model_name_or_path,
            **method_kwargs,
        )
    elif method == "hf_prompt_tuning_11b":
        peft_config = peft.PromptTuningConfig(
            task_type=task_type,
            tokenizer_name_or_path=args.model_name_or_path,
            **method_kwargs,
        )
    else:
        raise ValueError(f"Can't find PEFT config object for method {method}, config string={args.adapter_config_string}")

    model = peft.get_peft_model(model, peft_config)
    model = model.to(device=device, dtype=args.torch_dtype)
    return model


@torch.no_grad()
def evaluate_model(
    model,
    *,
    metric,
    tokenizer,
    dataloader,
    accelerator,
    postprocess_fn,
    target_length,
    max_length=None,
    num_beams=None,
    max_iters=None,
    decoder_only=False,
):
    """
    Args:
        postprocess_fn: a function that takes a pair of lists of strings (predictions, labels) and returns a pair of
            lists of strings (predictions, labels) after postprocessing.
            For an example, look at `peft_comparison.text2text_utils.postprocess_summarization`
    """
    _time = time.time()
    model.eval()
    pbar = tqdm(
        dataloader,
        desc="Evaluating",
        disable=not accelerator.is_local_main_process,
        total=max_iters or len(dataloader),
        ncols=80,
    )
    peak_memory_usage = 0
    current_memory_usage = 0
    batch_start_time = time.time()
    throughput_tokens = 0
    throughput_examples = 0
    throughput_batches = 0
    first_10_predictions = []
    first_10_labels = []

    throughput_tokens_list = []
    prediction_lengths_list = []

    synced_gpus = None
    _ds_plugin = accelerator.state.deepspeed_plugin
    if _ds_plugin is not None and _ds_plugin.zero_stage == 3:
        synced_gpus = True
        logger.info("Running in stage 3, will use synced gpus for generation")

    for eval_step, batch in enumerate(dataloader):
        pbar.update()
        if max_iters is not None and eval_step > max_iters:
            logger.info(f"{max_iters} evaluation steps reached. Stopping evaluation.")
            break

        unwrapped_model = accelerator.unwrap_model(model)

        batch_start_time = time.time()
        generated_tokens = unwrapped_model.generate(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            max_length=max_length,
            max_new_tokens=target_length,
            num_beams=num_beams,
            do_sample=False,
            synced_gpus=synced_gpus,  # stage 3 requires synced gpus or generation may stall
        )
        update_time = time.time() - batch_start_time

        # track memory usage and throughput
        peak_memory_usage += torch.cuda.max_memory_allocated() / (1024 ** 2)
        current_memory_usage += torch.cuda.memory_allocated() / (1024 ** 2)
        throughput_tokens += batch["input_ids"].numel() / update_time
        throughput_examples += batch["input_ids"].shape[0] / update_time
        throughput_batches += 1 / update_time

        throughput_tokens_list.append(batch["input_ids"].numel() / update_time)
        if decoder_only:
            prediction_lengths_list.append(generated_tokens.shape[1] - batch["input_ids"].shape[1])
        else:
            prediction_lengths_list.append(generated_tokens.shape[1])

        if decoder_only:
            # replace token_ids corresponding to the input text (without the label text i.e. class label name or summary)
            generated_tokens = peft_comparison.text2text_utils.strip_input_tokens_from_generation(
                generated_tokens=generated_tokens,
                len_input_wo_class=[i["input_len"] for i in batch["metadata"]],
                pad_token_id=tokenizer.pad_token_id,
            )
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        ).cpu().numpy()

        if isinstance(generated_tokens, tuple):
            generated_tokens = generated_tokens[0]

        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        labels_str = [_l["targets"] for _l in batch["metadata"]]
        if len(decoded_preds) != len(labels_str):
            print(f"Input ids shape: {batch['input_ids'].shape}"),
            print(f"{len(decoded_preds)} != {len(labels_str)}")

        decoded_preds, labels_str = postprocess_fn(decoded_preds, labels_str)
        metric.add_batch(predictions=decoded_preds, references=labels_str)

        if len(first_10_predictions) < 10:
            first_10_predictions.extend(decoded_preds[:10])
            first_10_labels.extend(labels_str[:10])

        if eval_step == 0:
            logger.info("\n")
            logger.info(f"Example of predictions: {decoded_preds[0]}")
            logger.info(f"Example of labels: {labels_str[0]}")

    result = metric.compute()

    # save performance and other metrics to track memory consumption and throughput
    result = {f"eval/{k}": round(v * 100, 4) for k, v in result.items()}
    result["eval/peak_memory_usage"] = peak_memory_usage / (eval_step + 1)
    result["eval/current_memory_usage"] = current_memory_usage / (eval_step + 1)
    result["eval/throughput_tokens"] = throughput_tokens / (eval_step + 1)
    result["eval/throughput_examples"] = throughput_examples / (eval_step + 1)
    result["eval/throughput_batches"] = throughput_batches / (eval_step + 1)

    # throughput_tokens_mean is a sanity check that it equals to eval/throughput_tokens
    result["eval/throughput_tokens_mean"] = sum(throughput_tokens_list) / len(throughput_tokens_list)
    # skip the first one because it's likely an outlier that will increase the std quite a bit and won't estimate what we want to measure
    result["eval/throughput_tokens_std"] = torch.tensor(throughput_tokens_list[1:], dtype=torch.float32).std().item()
    result["eval/prediction_lengths_mean"] = sum(prediction_lengths_list) / len(prediction_lengths_list)
    result["eval/prediction_lengths_std"] = torch.tensor(prediction_lengths_list, dtype=torch.float32).std().item()

    logger.info("Logging predictions to wandb")
    if wandb.run is not None:
        table = wandb.Table(columns=["Targets", "Predictions"])
        for t, p in zip(first_10_labels, first_10_predictions):
            table.add_data(t, p)
        result["predicitons"] = table

        wandb.log({"eval/throughput_tokens_hist": wandb.Histogram(throughput_tokens_list)})
        wandb.log(result)

    model.train()
    logger.info(f"Evaluation took {time.time() - _time:.2f} seconds")
    torch.cuda.empty_cache()
    return result


def preprocess_function(
        examples,
        *,
        source_prefix,
        max_source_length,
        max_target_length,
        decoder_only,
        is_eval,
        tokenizer,
        padding,
    ):
    inputs = examples["source_text"]
    targets = examples["target_text"]
    inputs = [source_prefix + inp for inp in inputs]

    if not decoder_only:
        # T5
        model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)
        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)
        if padding == "max_length":
            labels["input_ids"] = [
                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
            ]
        model_inputs["labels"] = labels["input_ids"]
        if is_eval:
            model_inputs["metadata"] = [{"targets": t} for t in targets]
    else:
        # @NOTE: the way we have written preprocessing and collation for llama,
        # - set padding=False in preprocessing (so that we know what's the max_len in the batch)
        # - set padding=True in collation (so that we can pad to the multiple of 8 > max_len in the batch)
        # - we can set labels to input_ids because the token shifting is taken care of in the modeling_llaama file

        if is_eval:
            model_inputs = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=True)
        else:
            inputs = [i + " " + t for i, t in zip(inputs, targets)]
            model_inputs = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=True)
        model_inputs["labels"] = model_inputs["input_ids"]
        if is_eval:
            input_wo_label = tokenizer(inputs, max_length=max_source_length, padding=False, truncation=False)
            input_wo_label = input_wo_label["input_ids"]
            model_inputs["metadata"] = []
            for idx in range(len(targets)):
                model_inputs["metadata"].append({
                    "targets": targets[idx],
                    "input_len": len(input_wo_label[idx]),
                })

    return model_inputs


def main():
    args = parse_args()

    if args.seed is not None:
        set_seed(args.seed)

    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers
    # in the environment
    accelerator = Accelerator(project_dir=args.output_dir, log_with="wandb")
    if not accelerator.is_main_process:
        logger.remove()

    logger.info(f"Accelerator state: {accelerator.state}")

    # It's important to get this after the accelerator initialization
    device = torch.cuda.current_device()
    args.device = device

    if args.total_batch_size is not None:
        if args.gradient_accumulation_steps is not None:
            logger.warning("`--total_batch_size` overrides --gradient_accumulation_steps")
        if args.total_batch_size % (accelerator.num_processes * args.per_device_train_batch_size) != 0:
            raise ValueError(f"`--total_batch_size` ({args.total_batch_size}) is not divisible by "
                             f"num_processes * per_device_train_batch_size ({accelerator.num_processes} * {args.per_device_train_batch_size})")
        logger.info(f"accelerator.num_processes: {accelerator.num_processes}")
        args.gradient_accumulation_steps = args.total_batch_size // (args.per_device_train_batch_size * accelerator.num_processes)
        logger.info(f"Setting gradient accumulation steps to {args.gradient_accumulation_steps}.")
    else:
        args.total_batch_size = args.gradient_accumulation_steps * args.per_device_train_batch_size * accelerator.num_processes

    if accelerator.is_main_process and args.output_dir is not None:
        os.makedirs(args.output_dir, exist_ok=True)

    accelerator.gradient_accumulation_steps = args.gradient_accumulation_steps
    if accelerator.state.deepspeed_plugin is not None:
        logger.info(f"Setting gradient accumulation steps to {args.gradient_accumulation_steps} for deepspeed")
        accelerator.state.deepspeed_plugin.deepspeed_config["gradient_accumulation_steps"] = args.gradient_accumulation_steps

    accelerator.wait_for_everyone()

    accelerator.init_trackers(
        args.wandb_project,
        init_kwargs={"wandb": {
            "tags": args.tags,
            "entity": args.wandb_team,
        }})

    if accelerator.is_main_process:
        wandb.save(os.path.abspath(__file__), policy="now") # save current script

    logger.info("*" * 40)
    for k, v in vars(args).items():
        logger.info(f"{k:30}: {v}")
    logger.info("*" * 40)

    mem = psutil.virtual_memory()
    pretty_output = (
        f"(CPU RAM) Total: {mem.total / (1024 ** 3):.2f} GB, "
        f"Available: {mem.available / (1024 ** 3):.2f} GB, "
        f"Used: {mem.used / (1024 ** 3):.2f} GB, "
        f"Percentage: {mem.percent}%"
    )
    logger.info(pretty_output)

    # tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name_or_path,
        model_max_length=max(args.max_source_length, args.max_target_length) + 2,  # void the annoying warning messge, +2 is for bos and eos
    )

    logger.info("Loading model")
    # Load pretrained model and tokenizer
    use_adapters_library = not args.adapter_config_string.startswith("hf_")
    if use_adapters_library:
        logger.info("Using get_model (adapters library)")
        model = get_model(args, device=device)
    else:
        logger.info("Using get_model_hf")
        model = get_model_hf(args, device=device)
    torch.cuda.reset_peak_memory_stats()

    if model.config.decoder_start_token_id is None:
        model.config.decoder_start_token_id = tokenizer.bos_token_id

    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})
        model.config.pad_token_id = model.config.eos_token_id

    embedding_size = model.get_input_embeddings().weight.shape[0]
    if len(tokenizer) != embedding_size:
        logger.warning(f"Tokenizer vocab size ({len(tokenizer)}) does not match the model "
                       f"embedding size ({embedding_size}).")
        if len(tokenizer) > embedding_size:
            raise ValueError("Tokenizer vocab size is larger than the model embedding size. "
                             "Probably you're using a wrong tokenizer/model combination. ")

    logger.info(model)

    dtype_counts = {}
    for p in model.parameters():
        dtype_counts[str(p.dtype)] = dtype_counts.get(str(p.dtype), 0) + p.numel()

    if len(dtype_counts) == 0:
        logger.warning("No parameters in the model?")

    logger.info(dtype_counts)

    total_parameters = sum(dtype_counts.values())
    dtype_info = [f"{dtype}: {count} ({count / total_parameters * 100:.2f}%)" for dtype, count in dtype_counts.items()]
    logger.info("Model dtypes: " + " | ".join(dtype_info))

    # we need to add padding token to llama
    if args.decoder_only:
        tokenizer.add_special_tokens({'pad_token': tokenizer.bos_token})

    ############################################
    # Data preprocessing
    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)
    if args.subsample_data is not None:
        logger.warning(f"Subsampling the dataset to {args.subsample_data} first examples.")
        # remember that it's dataset dict
        def get_subsample_data(subset):
            return min(args.subsample_data, len(subset))

        raw_datasets = {k: v.select(range(get_subsample_data(v))) for k, v in raw_datasets.items()}
        raw_datasets = datasets.DatasetDict(raw_datasets)

        if args.eval_throughput_estimation:
            logger.warning(f"Only initial eval will be performed, setting validation and test to train set to make sure we evaluate long enough")
            raw_datasets["validation"] = raw_datasets["train"]
            raw_datasets["test"] = raw_datasets["train"]

    _dataset_name_for_preprocessing = args.dataset_name
    if args.task_type == "classification":
        _dataset_name_for_preprocessing = args.dataset_config_name

    if args.subsample_data is None and args.dataset_name in ["cnn_dailymail", "EdinburghNLP/xsum"]:
        # 1600 is selected based on the MAM paper
        # sample 1600 examples for validation and test deterministically
        logger.warning(f"Subsampling the dataset to 1600 first examples for validation and test.")
        raw_datasets["validation"] = raw_datasets["validation"].select(range(1600))
        raw_datasets["test"] = raw_datasets["test"].select(range(1600))

    raw_datasets, postprocess_fn = peft_comparison.text2text_utils.dataset_to_text2text(
        raw_datasets,
        task_type=args.task_type,
        dataset_name=_dataset_name_for_preprocessing,
        decoder_only=args.decoder_only,
    )

    # Preprocessing the datasets.
    # First we tokenize all the texts.
    column_names = raw_datasets["train"].column_names
    preprocess_kwargs = {
        'source_prefix': args.source_prefix, 
        'max_source_length': args.max_source_length, 
        'max_target_length': args.max_target_length, 
        'decoder_only': args.decoder_only, 
        'is_eval': True, 
        'tokenizer': tokenizer,
        'padding': "max_length" if args.pad_to_max_length else False,
    }

    with accelerator.main_process_first():
        eval_dataset = raw_datasets["validation"].map(
            preprocess_function,
            batched=True,
            num_proc=args.preprocessing_num_workers,
            remove_columns=column_names,
            desc="Running tokenizer on val dataset  ",
            fn_kwargs=preprocess_kwargs,
        )
        test_dataset = eval_dataset
        if "test" in raw_datasets:
            test_dataset = raw_datasets["test"].map(
                preprocess_function,
                batched=True,
                num_proc=args.preprocessing_num_workers,
                remove_columns=column_names,
                fn_kwargs=preprocess_kwargs,
            )

        preprocess_kwargs["is_eval"] = False
        train_dataset = raw_datasets["train"].map(
            preprocess_function,
            batched=True,
            batch_size=min(5000, len(raw_datasets["train"]) // args.preprocessing_num_workers),
            num_proc=args.preprocessing_num_workers,
            remove_columns=column_names,
            desc="Running tokenizer on train dataset",
            fn_kwargs=preprocess_kwargs,
        )

    if len(raw_datasets["validation"]) < 1_000:
        logger.warning(f"Validation dataset is small ({raw_datasets['validation']}), running full validation set during training.")
        args.max_eval_steps_durig_validation = None

    # Log a few random samples from the training set:
    if args.verbosity > 1:
        for index in random.sample(range(len(train_dataset)), 1):
            logger.info(f"Sample {index} of the training set:")
            for k, v in train_dataset[index].items():
                if hasattr(v, "shape"):
                    logger.info(f"  {k}.shape: {v.shape}")
                logger.info(f"  {k}: {v}")

    label_pad_token_id = -100
    if not args.decoder_only:
        data_collator = DataCollatorForSeq2SeqWithMetadata(
            tokenizer,
            model=model,
            label_pad_token_id=label_pad_token_id,
            pad_to_multiple_of=8,
        )
    else:
        data_collator = DataCollatorForCausalLMWithMetadata(
            tokenizer=tokenizer,
            padding=True,
            max_length=args.max_source_length,
            pad_to_multiple_of=8,
        )

    train_dataloader = DataLoader(
        train_dataset,
        shuffle=True,
        collate_fn=data_collator,
        batch_size=args.per_device_train_batch_size
    )
    eval_dataloader = DataLoader(
        eval_dataset,
        collate_fn=data_collator,
        batch_size=args.per_device_eval_batch_size,
    )
    test_dataloader = eval_dataloader
    if "test" in raw_datasets:
        test_dataloader = DataLoader(
            test_dataset,
            collate_fn=data_collator,
            batch_size=args.per_device_eval_batch_size,
        )

    # Optimizer
    # Split weights in two groups, one with weight decay and the other not.
    no_decay = ["bias", "LayerNorm", "layer_norm"]
    optimizer_grouped_parameters = [
        {
            "params": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)],
            "weight_decay": args.weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],
            "weight_decay": 0.0,
        },
    ]
    total_parameters = sum(p.numel() for p in model.parameters())
    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
    args.total_parameters = total_parameters
    args.trainable_parameters = trainable_parameters
    logger.info(f"Total model parameters: {total_parameters:,}")
    logger.info(f"Trainable parameters  : {trainable_parameters:,} ({trainable_parameters / total_parameters * 100:.4f}%)")
    if args.verbosity > 1:
        logger.info("Trainable model parameters")
        for name, param in model.named_parameters():
            if param.requires_grad:
                logger.info(f"{name}: {param.numel():,}")

    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)
    if args.eval_throughput_estimation:
        del optimizer
        logger.warning("Skipping training because --eval_throughput_estimation is set. Using fake optimizer to avoid potential OOM errors")
        optimizer = torch.optim.AdamW([torch.zeros(1, requires_grad=True)], lr=1e-10)
        for param in model.parameters():
            param.requires_grad = False

    # Scheduler and math around the number of training steps.
    overrode_max_train_steps = False
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    if args.max_train_steps < args.min_train_steps:
        args.max_train_steps = args.min_train_steps
        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
        overrode_max_train_steps = True
        logger.warning(f"Overriding `max_train_steps` to {args.min_train_steps} "
                       f"and `args.num_train_epochs` to {args.num_train_epochs} "
                       f"to meet `min_train_steps` requirement.")

    # Prepare everything with our `accelerator`.
    model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader, test_dataloader
    )

    if args.eval_every_steps is None:
        args.eval_every_steps = min(2000, args.max_train_steps // 10)
        logger.info(f"Setting `eval_every_steps` to {args.eval_every_steps}.")

    # We need to recalculate our total training steps as the size of the training dataloader may have changed.
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch

    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=int(args.max_train_steps * args.lr_scheduler_warmup_percent),
        num_training_steps=args.max_train_steps,
    )

    # Afterwards we recalculate our number of training epochs
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # We need to initialize the trackers we use, and also store our configuration.
    # The trackers initializes automatically on the main process.
    experiment_config = vars(args)
    experiment_config["lr_scheduler_type"] = experiment_config["lr_scheduler_type"].value
    experiment_config["world_size"] = accelerator.num_processes
    experiment_config["gpu_type"] = torch.cuda.get_device_name(0)

    if accelerator.is_main_process:
        wandb.config.update(experiment_config)

    # Metric
    if args.task_type == "summarization":
        metric = evaluate.load("rouge")
    else:
        metric = evaluate.load("super_glue", args.dataset_config_name)

    # Train!
    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps

    logger.info("***** Running training *****")
    logger.info(f"  Num examples = {len(train_dataset)}")
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
    logger.info(f"  Total optimization steps = {args.max_train_steps}")
    logger.info(f"  Total warmup steps for LR = {int(args.max_train_steps * args.lr_scheduler_warmup_percent)}")

    # Only show the progress bar once on each machine.
    progress_bar = trange(
        args.max_train_steps,
        desc="Training (total steps)",
        disable=not accelerator.is_local_main_process,
        total=args.max_train_steps,
        ncols=80,
        leave=True,
    )

    update_step = 0
    starting_epoch = 0
    global_step = 0

    # Potentially load in the weights and states from a previous save
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != "":
            checkpoint_path = args.resume_from_checkpoint
            path = os.path.basename(args.resume_from_checkpoint)
        else:
            # Get the most recent checkpoint
            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]
            dirs.sort(key=os.path.getctime)
            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last
            checkpoint_path = path
            path = os.path.basename(checkpoint_path)

        logger.info(f"Resumed from checkpoint: {checkpoint_path}")
        accelerator.load_state(path)
        # Extract `epoch_{i}` or `step_{i}`
        training_difference = os.path.splitext(path)[0]

        if "epoch" in training_difference:
            starting_epoch = int(training_difference.replace("epoch_", "")) + 1
            resume_step = None
            update_step = starting_epoch * num_update_steps_per_epoch
        else:
            # need to multiply `gradient_accumulation_steps` to reflect real steps
            resume_step = int(training_difference.replace("step_", "")) * args.gradient_accumulation_steps
            starting_epoch = resume_step // len(train_dataloader)
            resume_step -= starting_epoch * len(train_dataloader)
            update_step = resume_step // args.gradient_accumulation_stepp

        progress_bar.update(update_step)

    active_dataloader = train_dataloader
    if args.resume_from_checkpoint and resume_step is not None:
        active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)

    tokens_seen = 0
    tokens_seen_before = 0
    batches_in_update = args.gradient_accumulation_steps * accelerator.num_processes
    main_metric_name = "eval/accuracy" if args.task_type == "classification" else "eval/rougeL"
    best_metric_value = 0
    best_results_dict = None

    prof = None
    if args.profile:
        prof = torch.profiler.profile(
                schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
                on_trace_ready=torch.profiler.tensorboard_trace_handler('./profiler_logs'),
                record_shapes=True,
                with_stack=True)

    for epoch in range(starting_epoch, args.num_train_epochs):
        logger.info(f"Starting epoch {epoch + 1} / {args.num_train_epochs}")
        model.train()

        if prof is not None: prof.start()
        for batch_idx, batch in enumerate(active_dataloader):
            global_step += 1
            if prof is not None: prof.step()
            # vars for calculating throughput
            batch_start_time = time.time()
            tokens_seen += batch["input_ids"].numel() * accelerator.num_processes

            # print first batch
            if batch_idx == 0 and epoch == 0:
                logger.info("============= CHECKING FIRST BATCH =============")
                logger.info("Tensor shapes: ")
                logger.info(batch["input_ids"].shape)
                logger.info("Decoded text of first example in the batch:")
                s_text = tokenizer.decode(batch["input_ids"][0, :], skip_special_tokens=False)
                logger.info(f"input_ids text: {s_text}")
                if "decoder_input_ids" in batch:
                    t_text = tokenizer.decode(batch["decoder_input_ids"][0, :], skip_special_tokens=False)
                    logger.info(f"decoder_input_ids text: {t_text}")

                t_ids = batch["labels"][0, :]
                t_ids[t_ids == -100] = tokenizer.pad_token_id
                t_text = tokenizer.decode(t_ids, skip_special_tokens=False)
                logger.info(f"Labels text: {t_text}")

            with accelerator.accumulate(model):
                outputs = model(**batch)
                loss = outputs.loss
                accelerator.backward(loss)

            if not accelerator.sync_gradients:
                continue
            # the code below is only executed on the update step

            progress_bar.update()
            optimizer.step()
            lr_scheduler.step()

            # log memory and thoughput
            if (update_step % 10) == 0:
                # examples per second and batches per second are not computed correctly
                tokens_in_update = tokens_seen - tokens_seen_before
                peak_memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 2)  # in megabytes
                current_memory_usage = torch.cuda.memory_allocated() / (1024 ** 2)
                update_time = time.time() - batch_start_time
                batch_start_time = time.time()
                accelerator.log(
                    {
                        "throughput_tokens": tokens_in_update / update_time,
                        "throughput_examples": args.total_batch_size / update_time,
                        "throughput_batches": batches_in_update / update_time,
                        "peak_memory_usage": peak_memory_usage,
                        "current_memory_usage": current_memory_usage,
                    },
                    step=update_step,
                )
                tokens_seen_before = tokens_seen

            # clear grads
            optimizer.zero_grad()
            update_step += 1

            if update_step >= args.max_train_steps:
                logger.info("Max number of steps reached. Stopping training")
                break

            accelerator.log(
                {
                    "train/loss": loss,
                    "lr": optimizer.param_groups[0]["lr"],
                    "epoch": epoch,
                    "update_step": update_step,
                    "global_steps": global_step,
                },
                step=update_step,
            )

            if (update_step + 1) % args.eval_every_steps == 0 or (update_step == 1 and not args.skip_initial_eval):
                logger.info(f"Evaluating model at, update step: {update_step}, global step: {global_step}")
                result = evaluate_model(
                    model=model,
                    metric=metric,
                    tokenizer=tokenizer,
                    dataloader=eval_dataloader,
                    accelerator=accelerator,
                    max_length=(args.max_source_length + args.max_target_length) if args.decoder_only else args.val_max_target_length,
                    target_length=args.val_max_target_length,
                    num_beams=1,
                    max_iters=args.max_eval_steps_durig_validation,
                    postprocess_fn=postprocess_fn,
                    decoder_only=args.decoder_only
                )
                torch.cuda.empty_cache()

                accelerator.log(result, step=update_step)
                if result[main_metric_name] > best_metric_value:
                    logger.info(f"New best metric found: {result[main_metric_name]} at update step {update_step}")
                    best_metric_value = result[main_metric_name]
                    best_results_dict = result.copy()

                if args.eval_throughput_estimation:
                    # Another thing this option does it using a fake optimizer (see before the training loop)
                    logger.info("Stopping training because `eval_throughput_estimation` is set.")
                    wandb.finish()
                    time.sleep(5)  # give wandb time to finish any async stuff it's doing
                    exit(0)

    # final evaluation
    # @NOTE: we want to do atleast one evaluation with beam search
    update_step += 1
    logger.info(f"Final evaluation, validation set, {update_step=}")

    result = evaluate_model(
        model=model,
        metric=metric,
        tokenizer=tokenizer,
        dataloader=eval_dataloader,
        accelerator=accelerator,
        max_length=(args.max_source_length + args.max_target_length) if args.decoder_only else args.val_max_target_length,
        target_length=args.val_max_target_length,
        num_beams=args.num_beams,
        max_iters=None,
        postprocess_fn=postprocess_fn,
        decoder_only=args.decoder_only
    )
    if result[main_metric_name] >= best_metric_value:
        logger.info(f"New best metric found: {result[main_metric_name]} at update step {update_step}")
        best_metric_value = result[main_metric_name]
        best_results_dict = result.copy()

    logger.info(pformat(result))
    accelerator.log(result, step=update_step)

    if best_results_dict is not None:
        accelerator.log(best_results_dict, step=update_step)
    else:
        logger.warning(f"best_results_dict is None, no evaluation was done during training")

    # Eval on test set
    if "test" in raw_datasets and not args.skip_test_eval:
        logger.info(f"Final evaluation, test set, {update_step=}")
        result = evaluate_model(
            model=model,
            metric=metric,
            tokenizer=tokenizer,
            dataloader=test_dataloader,
            accelerator=accelerator,
            max_length=(args.max_source_length + args.max_target_length) if args.decoder_only else args.val_max_target_length,
            target_length=args.val_max_target_length,
            num_beams=args.num_beams,
            max_iters=None,
            postprocess_fn=postprocess_fn,
            decoder_only=args.decoder_only
        )
        result = {f"test/{k.lstrip('eval/')}": v for k, v in result.items()}
        accelerator.log(result, step=update_step)

    # save results and all arguments
    all_results = best_results_dict.copy()
    all_results["args"] = vars(args)
    for k, v in all_results["args"].items():
        if not isinstance(v, (float, int, bool, str, list)):
            all_results["args"][k] = str(v)

    print(f"Saving results to {args.output_dir}")
    if os.path.exists(args.output_dir):
        with open(os.path.join(args.output_dir, "all_results.json"), "w") as f:
            _all_results = {k: v for k, v in all_results.items() if not isinstance(v, wandb.Table)}
            json.dump(_all_results, f, indent=4)

    logger.info("Script successfully finished!")


if __name__ == "__main__":
    main()
